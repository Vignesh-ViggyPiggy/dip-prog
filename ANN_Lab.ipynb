{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Program 1**"
      ],
      "metadata": {
        "id": "8xOdCYaLr6hq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CB1yvzZarJo0",
        "outputId": "2068f630-7d54-4028-9224-208827cd7a7a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction for input [1, 1]: 1\n"
          ]
        }
      ],
      "source": [
        "# Simple Perceptron Example\n",
        "\n",
        "# Step function to make decisions (activation function)\n",
        "def step_function(value):\n",
        "    return 1 if value >= 0 else 0\n",
        "\n",
        "# Perceptron learning algorithm\n",
        "def perceptron(X, y, learning_rate, n_iters):\n",
        "    weights = [0, 0]  # Initialize weights for two features\n",
        "    bias = 0          # Initialize bias\n",
        "\n",
        "    # Training loop\n",
        "    for _ in range(n_iters):\n",
        "        for i in range(len(X)):\n",
        "            # Calculate weighted sum\n",
        "            weighted_sum = X[i][0] * weights[0] + X[i][1] * weights[1] + bias\n",
        "            prediction = step_function(weighted_sum)\n",
        "\n",
        "            # Update rule\n",
        "            error = y[i] - prediction\n",
        "            weights[0] += learning_rate * error * X[i][0]\n",
        "            weights[1] += learning_rate * error * X[i][1]\n",
        "            bias += learning_rate * error\n",
        "\n",
        "    return weights, bias\n",
        "\n",
        "# Training data: AND gate logic\n",
        "X = [[0, 0], [0, 1], [1, 0], [1, 1]]  # Input features\n",
        "y = [0, 0, 0, 1]  # Expected output (AND gate)\n",
        "\n",
        "# Train the perceptron\n",
        "learning_rate = 0.1\n",
        "n_iters = 10\n",
        "weights, bias = perceptron(X, y, learning_rate, n_iters)\n",
        "\n",
        "# Predict for a new input\n",
        "def predict(X, weights, bias):\n",
        "    weighted_sum = X[0] * weights[0] + X[1] * weights[1] + bias\n",
        "    return step_function(weighted_sum)\n",
        "\n",
        "# Test the perceptron\n",
        "test_input = [1, 1]\n",
        "print(\"Prediction for input [1, 1]:\", predict(test_input, weights, bias))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Program 2**"
      ],
      "metadata": {
        "id": "zIPJZAkeXY3P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Inputs and step function\n",
        "INPUTS = np.array([[1, 1], [1, -1], [-1, 1], [-1, -1]])\n",
        "\n",
        "def step_function(sum): return 1 if sum >= 0 else -1\n",
        "\n",
        "def calculate_output(weights, instance, bias): return step_function(np.dot(instance, weights) + bias)\n",
        "\n",
        "# Hebbian Learning Algorithm\n",
        "def hebb(outputs):\n",
        "    weights, bias = np.zeros(2), 0  # Initialize weights and bias\n",
        "    for i in range(len(outputs)):\n",
        "        weights += INPUTS[i] * outputs[i]\n",
        "        bias += outputs[i]\n",
        "    return weights, bias\n",
        "\n",
        "# Train, test, and print results for both AND and OR gates\n",
        "def train_and_print(gate_name, outputs):\n",
        "    weights, bias = hebb(outputs)\n",
        "    print(f\"\\n{gate_name.upper()} Gate:\")\n",
        "    for input_vec in INPUTS:\n",
        "        output = calculate_output(weights, input_vec, bias)\n",
        "        print(f\"Input: {input_vec}, Output: {output}\")\n",
        "\n",
        "# AND and OR gate outputs\n",
        "and_outputs = np.array([1, -1, -1, -1])\n",
        "or_outputs = np.array([1, 1, 1, -1])\n",
        "\n",
        "# Print results for both gates\n",
        "train_and_print(\"AND\", and_outputs)\n",
        "train_and_print(\"OR\", or_outputs)"
      ],
      "metadata": {
        "id": "Ew5AoAKmXYNP",
        "outputId": "e1df61bc-6dca-44f6-c356-77779bf1c6aa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "AND Gate:\n",
            "Input: [1 1], Output: 1\n",
            "Input: [ 1 -1], Output: -1\n",
            "Input: [-1  1], Output: -1\n",
            "Input: [-1 -1], Output: -1\n",
            "\n",
            "OR Gate:\n",
            "Input: [1 1], Output: 1\n",
            "Input: [ 1 -1], Output: 1\n",
            "Input: [-1  1], Output: 1\n",
            "Input: [-1 -1], Output: -1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Program 3**"
      ],
      "metadata": {
        "id": "MOAzw7iEYTgy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "# Generate a larger synthetic dataset (100 crabs with features: shell width, claw size, weight)\n",
        "np.random.seed(100)\n",
        "blue_crabs = np.random.normal([5.5, 3.0, 0.4], 0.5, (50, 3))  # 50 Blue crabs\n",
        "orange_crabs = np.random.normal([6.0, 3.5, 0.5], 0.5, (50, 3)) # 50 Orange crabs\n",
        "\n",
        "# Combine the data and create labels (0 = Blue, 1 = Orange)\n",
        "data = np.vstack((blue_crabs, orange_crabs))\n",
        "labels = np.array([0] * 50 + [1] * 50)\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Build and compile the neural network (PatternNet)\n",
        "model = Sequential([\n",
        "    Dense(5, input_dim=3, activation='relu'),  # 3 input features (shell width, claw size, weight)\n",
        "    Dense(5, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')  # Output layer for binary classification\n",
        "])\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.fit(X_train, y_train, epochs=50, batch_size=4, verbose=0)  # Train the model\n",
        "\n",
        "# Evaluate the model on the test data\n",
        "loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "# Predict species for a new crab (e.g., shell width, claw size, weight)\n",
        "new_crab = np.array([[5.9, 3.3, 0.55]])\n",
        "prediction = (model.predict(new_crab) > 0.5).astype(int)\n",
        "species = [\"Blue\", \"Orange\"]\n",
        "print(f\"The predicted species for the new crab is: {species[prediction[0][0]]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j43ZsJutYWU7",
        "outputId": "c422ddda-4019-459c-95d9-797d6ee7d2fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_test_function.<locals>.one_step_on_iterator at 0x7eeb23962200> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7eeb23961bd0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 50.00%\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step\n",
            "The predicted species for the new crab is: Orange\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Program 4**"
      ],
      "metadata": {
        "id": "k9M9Lsp1ditt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Data\n",
        "X, y = load_wine(return_X_y=True)\n",
        "X = StandardScaler().fit_transform(X)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "# Model\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(X_train.shape[1], 10),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(10, 3)\n",
        ")\n",
        "\n",
        "# Training\n",
        "X_train, y_train = torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.long)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "for _ in range(1000):\n",
        "    optimizer.zero_grad()\n",
        "    loss = loss_fn(model(X_train), y_train)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# Evaluation\n",
        "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
        "_, predicted = torch.max(model(X_test), 1)\n",
        "accuracy = (predicted == torch.tensor(y_test)).float().mean().item()\n",
        "\n",
        "print(f'Accuracy: {accuracy * 100:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y4ABz4E4diH4",
        "outputId": "9f1531d9-d456-472e-944d-d75065333613"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 91.67%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Program 5**"
      ],
      "metadata": {
        "id": "c34_VcvlgIIW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Define operations as lambda functions\n",
        "add = lambda a, b: a + b\n",
        "sub = lambda a, b: a - b\n",
        "mul = lambda a, b: a * b\n",
        "div = lambda a, b: a / b if (b != 0).all() else \"Error! Division by zero.\"\n",
        "\n",
        "# Example usage\n",
        "x, y = torch.tensor([10.0]), torch.tensor([5.0])\n",
        "\n",
        "print(f\"Add: {add(x, y).item()}\")\n",
        "print(f\"Sub: {sub(x, y).item()}\")\n",
        "print(f\"Mul: {mul(x, y).item()}\")\n",
        "print(f\"Div: {div(x, y)}\")"
      ],
      "metadata": {
        "id": "Sja78MNigMhY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Program 6**"
      ],
      "metadata": {
        "id": "yWodJWfkgz_H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Data and labels\n",
        "X = np.array([[2, 3], [1, 1], [2, 1], [3, 3], [2, 2]])\n",
        "y = np.array([1, -1, -1, 1, -1])\n",
        "\n",
        "# LMS algorithm\n",
        "w, b, lr = np.zeros(2), 0, 0.01\n",
        "for _ in range(1000):\n",
        "    for i in range(len(X)):\n",
        "        y_pred = np.dot(X[i], w) + b\n",
        "        error = y[i] - y_pred\n",
        "        w += lr * error * X[i]\n",
        "        b += lr * error\n",
        "\n",
        "# Prediction\n",
        "pred = np.sign(np.dot(X, w) + b)\n",
        "print(f\"Final Weights: {w}, Bias: {b}, Predictions: {pred}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gUlq4xQrg3vK",
        "outputId": "558ce95e-55e3-4ebc-ca57-316b6b6aaa7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Weights: [-0.06145498  1.01651104], Bias: -2.1441048607297626, Predictions: [ 1. -1. -1.  1. -1.]\n"
          ]
        }
      ]
    }
  ]
}